{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#print(dataset[\\'Payer\\'].unique)\\ndataseta.drop(dataseta[dataseta[\\'Payer\\']==\\'*\\'].index,inplace=True)\\n\\ndataseta.drop(columns=\\'PercentofPayAmount\\',inplace=True)\\n\\ndataset=dataseta[dataseta.duplicated(subset=[\"Payer\",\"Debit_CreditInd.\",\"DaystoPay\"],keep=\\'last\\')] \\ndatasetaa=dataseta[dataseta[\\'Debit_CreditInd.\\']==\\'S\\']\\nprint(datasetaa.columns)\\ndatasetaa=datasetaa[[\\'Payer\\',\\'DaystoPay\\']]\\nconvert_dict = {\\'Payer\\': object, \\n                \\'DaystoPay\\': float\\n               } \\n  \\ndatasetaa = datasetaa.astype(convert_dict) \\ndatasetcleansed=datasetaa.groupby(\\'Payer\\')[\\'DaystoPay\\'].mean()\\ndatasetcleansed[\\'Payer\\']=datasetcleansed.index\\nprint(datasetcleansed)\\n\\nX1=dataset[[\\'Payer\\',\\'Debit_CreditInd.\\']]\\ndataset_test=dataset_test1\\nX1[\\'Debit_CreditInd.\\']=dataset[\\'Debit_CreditInd.\\'].apply(lambda x: 1 if x == \\'S\\' else 0)\\ndataset_test[\\'Debit_CreditInd.\\']=dataset_test[\\'Debit_CreditInd.\\'].apply(lambda x: 1 if x == \\'S\\' else 0)\\ny = dataset.iloc[:, -1].values\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfile = open(\"models.pkl\", \"wb\")\\nct = ColumnTransformer(transformers=[(\\'encoder\\', OneHotEncoder(sparse=False,handle_unknown=\\'ignore\\'), [0])], remainder=\\'passthrough\\')\\nX = np.array(ct.fit_transform(X1))\\npk.dump(ct, file)\\nfrom sklearn.ensemble import RandomForestClassifier\\nclassifier1 = RandomForestClassifier(n_estimators = 75, criterion = \\'entropy\\', random_state = 0,bootstrap=True,\\n                                    ccp_alpha=0.0, class_weight=None,\\n                        max_depth=None, max_features=\\'auto\\',\\n                       max_leaf_nodes=None, max_samples=None,\\n                       min_impurity_decrease=0.0, min_impurity_split=None,\\n                       min_samples_leaf=1, min_samples_split=2,\\n                       min_weight_fraction_leaf=0.0, \\n                       n_jobs=None, oob_score=False, verbose=0,\\n                       warm_start=False)\\nclassifier1.fit(X, y)\\npk.dump(classifier1, file)\\nfile.close()\\n\\nfile = open(\"models.pkl\", \"rb\")\\ntrained_encoder = pk.load(file)  #Pickle file first load the OneHotEncoder \\ntrained_model_for_prediction = pk.load(file)\\nX_test_encoded=np.array(trained_encoder.transform(dataset_test))\\ny_predict=trained_model_for_prediction.predict(X_test_encoded)\\ndataset_test[\\'DaystoPay\\']=y_predict\\nprint(dataset_test)\\nfile.close()\\ny_pred=classifier1.predict(X)\\n# Making the Confusion Matrix\\nfrom sklearn.metrics import confusion_matrix, accuracy_score\\nfrom sklearn.metrics import average_precision_score,classification_report\\nfrom sklearn.metrics import f1_score\\n#average_precision = average_precision_score(y_test_in, y_pred)\\ncm = confusion_matrix(y,y_pred)\\nprint( accuracy_score(y,y_pred))\\nprint(classification_report(y,y_pred))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "\n",
    "dataseta = pd.read_csv('Train.tsv',sep='\\t',dtype=object)\n",
    "dataseta.drop(dataseta[dataseta['Payer']=='*'].index,inplace=True)\n",
    "dataseta.drop(columns='PercentofPayAmount',inplace=True)\n",
    "dataseta=dataseta[dataseta['Debit_CreditInd.']=='S']\n",
    "print(dataseta.head())\n",
    "dataset_test1 = pd.read_csv('Test_S.tsv',sep='\\t',dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#print(dataset['Payer'].unique)\n",
    "dataseta.drop(dataseta[dataseta['Payer']=='*'].index,inplace=True)\n",
    "\n",
    "dataseta.drop(columns='PercentofPayAmount',inplace=True)\n",
    "\n",
    "dataset=dataseta[dataseta.duplicated(subset=[\"Payer\",\"Debit_CreditInd.\",\"DaystoPay\"],keep='last')] \n",
    "datasetaa=dataseta[dataseta['Debit_CreditInd.']=='S']\n",
    "print(datasetaa.columns)\n",
    "datasetaa=datasetaa[['Payer','DaystoPay']]\n",
    "convert_dict = {'Payer': object, \n",
    "                'DaystoPay': float\n",
    "               } \n",
    "  \n",
    "datasetaa = datasetaa.astype(convert_dict) \n",
    "datasetcleansed=datasetaa.groupby('Payer')['DaystoPay'].mean()\n",
    "datasetcleansed['Payer']=datasetcleansed.index\n",
    "print(datasetcleansed)\n",
    "\n",
    "X1=dataset[['Payer','Debit_CreditInd.']]\n",
    "dataset_test=dataset_test1\n",
    "X1['Debit_CreditInd.']=dataset['Debit_CreditInd.'].apply(lambda x: 1 if x == 'S' else 0)\n",
    "dataset_test['Debit_CreditInd.']=dataset_test['Debit_CreditInd.'].apply(lambda x: 1 if x == 'S' else 0)\n",
    "y = dataset.iloc[:, -1].values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "file = open(\"models.pkl\", \"wb\")\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(sparse=False,handle_unknown='ignore'), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X1))\n",
    "pk.dump(ct, file)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier1 = RandomForestClassifier(n_estimators = 75, criterion = 'entropy', random_state = 0,bootstrap=True,\n",
    "                                    ccp_alpha=0.0, class_weight=None,\n",
    "                        max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, \n",
    "                       n_jobs=None, oob_score=False, verbose=0,\n",
    "                       warm_start=False)\n",
    "classifier1.fit(X, y)\n",
    "pk.dump(classifier1, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"models.pkl\", \"rb\")\n",
    "trained_encoder = pk.load(file)  #Pickle file first load the OneHotEncoder \n",
    "trained_model_for_prediction = pk.load(file)\n",
    "X_test_encoded=np.array(trained_encoder.transform(dataset_test))\n",
    "y_predict=trained_model_for_prediction.predict(X_test_encoded)\n",
    "dataset_test['DaystoPay']=y_predict\n",
    "print(dataset_test)\n",
    "file.close()\n",
    "y_pred=classifier1.predict(X)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import average_precision_score,classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "#average_precision = average_precision_score(y_test_in, y_pred)\n",
    "cm = confusion_matrix(y,y_pred)\n",
    "print( accuracy_score(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

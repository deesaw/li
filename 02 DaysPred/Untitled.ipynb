{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88857 entries, 134 to 88990\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Payer               88857 non-null  object\n",
      " 1   Debit_CreditInd.    88857 non-null  object\n",
      " 2   PercentofPayAmount  88857 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Payer</th>\n",
       "      <th>Debit_CreditInd.</th>\n",
       "      <th>PercentofPayAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88857</td>\n",
       "      <td>88857</td>\n",
       "      <td>88857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>447</td>\n",
       "      <td>2</td>\n",
       "      <td>34010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>301088</td>\n",
       "      <td>S</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>25137</td>\n",
       "      <td>81277</td>\n",
       "      <td>13891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Payer Debit_CreditInd. PercentofPayAmount\n",
       "count    88857            88857              88857\n",
       "unique     447                2              34010\n",
       "top     301088                S                100\n",
       "freq     25137            81277              13891"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "dataseta = pd.read_csv('Train.tsv',sep='\\t',dtype=object)\n",
    "dataset_test1 = pd.read_csv('Test.tsv',sep='\\t',dtype=object)\n",
    "dataseta.drop(dataseta[dataseta['Payer']=='*'].index,inplace=True)\n",
    "dataseta.drop(columns='DaystoPay',inplace=True)\n",
    "dataseta.info()\n",
    "dataseta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payer                  object\n",
      "Debit_CreditInd.       object\n",
      "PercentofPayAmount    float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Convert_dict = {'Payer': str, \n",
    "                'Debit_CreditInd.': str,\n",
    "                'PercentofPayAmount': float\n",
    "               } \n",
    "  \n",
    "dataseta = dataseta.astype(Convert_dict) \n",
    "print(dataseta.dtypes) \n",
    "dataseti=dataseta.groupby(['Payer','Debit_CreditInd.'])['PercentofPayAmount'].mean()\n",
    "datasetii = pd.DataFrame(dataseti).reset_index()\n",
    "inner_join = pd.merge(dataseta,  \n",
    "                      datasetii,  \n",
    "                      on =['Payer','Debit_CreditInd.'],  \n",
    "                      how ='inner') \n",
    "inner_join['rank'] = inner_join.groupby(['Payer','Debit_CreditInd.'])['PercentofPayAmount_x'].rank()\n",
    "dataset_ready=inner_join[['Payer','Debit_CreditInd.','rank','PercentofPayAmount_x']]\n",
    "dataset=dataset_ready\n",
    "Convert_dict1 = {'Payer': str, \n",
    "                'Debit_CreditInd.': str,\n",
    "                'rank':int,\n",
    "                'PercentofPayAmount_x': str\n",
    "               } \n",
    "  \n",
    "dataset = dataset.astype(Convert_dict1)\n",
    "#dataset=dataset.head(50000)\n",
    "# Feature Scaling\n",
    "\n",
    "X1=dataset[['Payer','Debit_CreditInd.','rank']]\n",
    "dataset_ready.head()\n",
    "\n",
    "dataset_test=dataset_test1\n",
    "y = dataset.iloc[:, -1].values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(sparse=False,handle_unknown='ignore'), [0,1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X1))\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier1 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0,bootstrap=True,\n",
    "                                    ccp_alpha=0.0, class_weight=None,\n",
    "                        max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, \n",
    "                       n_jobs=None, oob_score=False, verbose=0,\n",
    "                       warm_start=False)\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier1 = KNeighborsClassifier(n_neighbors = 20,weights='uniform', metric = 'minkowski', p = 2)\n",
    "classifier1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Payer Debit_CreditInd.  rank PercentofPayAmount\n",
      "0     100030                H     1              100.0\n",
      "1     100030                S     1              100.0\n",
      "2   70000010                S     1        2.298850575\n",
      "3     300767                S     1        2.298850575\n",
      "4     100040                H     1              100.0\n",
      "5     100059                H     1              100.0\n",
      "6     301096                S     1              100.0\n",
      "7     301097                S     1        2.298850575\n",
      "8     301136                S     1        2.298850575\n",
      "9   70000000                S     1        2.298850575\n",
      "10    300529                H     1              100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Making the Confusion Matrix\\nfrom sklearn.metrics import confusion_matrix, accuracy_score\\nfrom sklearn.metrics import average_precision_score,classification_report\\nfrom sklearn.metrics import f1_score\\n#average_precision = average_precision_score(y_test_in, y_pred)\\ncm = confusion_matrix(y,y_pred)\\nprint( accuracy_score(y,y_pred))\\nprint(classification_report(y,y_pred))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['rank']=1\n",
    "X_test_encoded=np.array(ct.transform(dataset_test))\n",
    "y_predict=classifier1.predict(X_test_encoded)\n",
    "dataset_test['PercentofPayAmount']=y_predict\n",
    "print(dataset_test)\n",
    "y_pred=classifier1.predict(X)\n",
    "'''\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import average_precision_score,classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "#average_precision = average_precision_score(y_test_in, y_pred)\n",
    "cm = confusion_matrix(y,y_pred)\n",
    "print( accuracy_score(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier2 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0,bootstrap=True,\n",
    "                                    ccp_alpha=0.0, class_weight=None,\n",
    "                        max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, \n",
    "                       n_jobs=None, oob_score=False, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "classifier2.fit(X, y)\n",
    "dataset_test['rank']=1\n",
    "X_test_encoded=np.array(ct.transform(dataset_test))\n",
    "y_predict=classifier2.predict(X_test_encoded)\n",
    "dataset_test['PercentofPayAmount']=y_predict\n",
    "print(dataset_test)\n",
    "y_pred=classifier2.predict(X)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import average_precision_score,classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "#average_precision = average_precision_score(y_test_in, y_pred)\n",
    "cm = confusion_matrix(y,y_pred)\n",
    "print( accuracy_score(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
